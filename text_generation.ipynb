{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Generation\n",
    "mkultra is set up for convenient text generation with transformers generation and sampling tools.\n",
    "\n",
    "You can load a SoftPrompt object and simply concatenate it into your context string, where it shows up as a human-readable tag that tokenizes to the underlying number of tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "!pip install transformers\n",
    "#!pip install git+git://github.com/corolla-johnson/mkultra.git#egg=mkultra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.models.inference import GPT2SoftPromptLM\n",
    "from mkultra.tokenizers import GPT2SPTokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to instantiate mkultra's model and tokenizer classes.\n",
    "model = GPT2SoftPromptLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2SPTokenizerFast.from_pretrained(\"gpt2\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftPrompts may be loaded in one of several ways.\n",
    "# sp = SoftPrompt.from_file(\"soft_prompts/neuromancer.pt\")\n",
    "# sp = SoftPrompt.from_inputs_embeds(inputs_embeds)\n",
    "# sp = SoftPrompt.from_tuning_model(model)\n",
    "# We will instantiate an SP from a string for testing. This should behave identically to the text.\n",
    "sp = SoftPrompt.from_string(\"With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.\",\n",
    "                            model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about an sp can be printed with\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'get_tag_str()' to get a tag string that you can add to your context.\n",
    "prompt = sp.get_tag_str() + \" She\"\n",
    "\n",
    "# The addition operator also works:\n",
    "# prompt = sp + \" She\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The tag string conveniently contains the SP's name, part of its GUID,\n",
    "# and a series of '@'s which represent individual soft tokens.\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These tokens help you budget your context.\n",
    "prompt_len = len(tokenizer.encode(prompt))\n",
    "print(f\"Length of soft prompt: {len(tokenizer.encode(sp.get_tag_str()))}\")\n",
    "print(f\"Length of full prompt: {prompt_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation is as usual.\n",
    "output = generator( prompt,\n",
    "                    do_sample=True,\n",
    "                    min_length=prompt_len+100,\n",
    "                    max_length=prompt_len+100,\n",
    "                    repetition_penalty=1.7,\n",
    "                    top_p=0.8,\n",
    "                    temperature=0.7,\n",
    "                    use_cache=True,\n",
    "                    return_full_text=True)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is also fine to use generate() instead of a pipeline\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model.generate(input_ids)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decide to write your own sampler instead of a pipeline,\n",
    "# be aware that the special token logits will be very high.\n",
    "# This is accounted for when using model.generate() or pipelines,\n",
    "# but you will need to exclude special tokens when sampling.\n",
    "\n",
    "output = model(input_ids)"
   ]
  }
 ]
}