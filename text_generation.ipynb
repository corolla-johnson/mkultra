{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
   }
  },
  "interpreter": {
   "hash": "3a36cc62f71b170ca22994dbd401744aeca204aa470bb3afe779afe0ab68d530"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Text Generation\n",
    "mkultra is set up for convenient text generation with transformers generation and sampling tools.\n",
    "\n",
    "You can load a SoftPrompt object and simply concatenate it into your context string, where it shows up as a human-readable tag that tokenizes to the underlying number of tokens."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup for Colab only\n",
    "#!pip install transformers\n",
    "#!pip install git+https://github.com/corolla-johnson/mkultra.git#egg=mkultra --log PIP_LOG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.pipelines import pipeline\n",
    "from mkultra.inference import GPT2SoftPromptLM\n",
    "from mkultra.tokenizers import GPT2SPTokenizerFast\n",
    "from mkultra.soft_prompt import SoftPrompt\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You'll need to instantiate mkultra's model and tokenizer classes.\n",
    "model = GPT2SoftPromptLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = GPT2SPTokenizerFast.from_pretrained(\"gpt2\")\n",
    "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SoftPrompts may be loaded in several ways.\n",
    "# sp = SoftPrompt.from_json(json_str)\n",
    "# sp = SoftPrompt.from_inputs_embeds(inputs_embeds)\n",
    "# sp = SoftPrompt.from_tuning_model(model)\n",
    "sp = SoftPrompt.from_string(\"With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.\",\n",
    "                             model=model, tokenizer=tokenizer)\n",
    "\n",
    "#sp = SoftPrompt.from_file(\"sample_sps/finetune/neuromancer_gpt2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "FromString (2021-06-20 20:27:51.615463)\nLength: 22\nUUID:   9361b46c-0cfd-41cd-b418-2868fa735fb6\nDescription:\nCreated from string 'With the court firmly balkanized into three distinct factions, Princess Charlotte had her work cut out for her.'\n"
     ]
    }
   ],
   "source": [
    "# Information about a SoftPrompt can be printed with\n",
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 'get_tag_str()' to get a tag string that you can add to your context.\n",
    "prompt = sp.get_tag_str() + \"She\"\n",
    "\n",
    "# The addition operator also works:\n",
    "# prompt = sp + \"Armitage\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<FromString-9361b46c-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>She\n"
     ]
    }
   ],
   "source": [
    "# The tag string conveniently contains the SP's name, part of its GUID,\n",
    "# and a series of '@'s which represent individual soft tokens.\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Length of soft prompt: 22\nLength of full prompt: 23\n"
     ]
    }
   ],
   "source": [
    "# The tag string tokenizes to the correct length budget your context.\n",
    "prompt_len = len(tokenizer.encode(prompt))\n",
    "print(f\"Length of soft prompt: {len(tokenizer.encode(sp.get_tag_str()))}\")\n",
    "print(f\"Length of full prompt: {prompt_len}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "<FromString-9361b46c-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>She had no choice but to join the rest of her family, a decision that would not be easy. She was going through an incredible process with Princess Charlotte and she wanted everything done right for herself.\"You can't go back in time,\" said Queen Elsa as they walked down Main Street into their apartment building on Sunday night - just before midnight when it became clear who's really behind this whole thing.\"\"I think I'm getting close enough now! Now we're all gonna get outta here!\"\"Yes\n"
     ]
    }
   ],
   "source": [
    "# Generation is as usual.\n",
    "output = generator( prompt,\n",
    "                    do_sample=True,\n",
    "                    min_length=prompt_len+100,\n",
    "                    max_length=prompt_len+100,\n",
    "                    repetition_penalty=1.7,\n",
    "                    top_p=0.8,\n",
    "                    temperature=0.7,\n",
    "                    use_cache=True,\n",
    "                    return_full_text=True)\n",
    "\n",
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "<FromString-9361b46c-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>She had to fight the evil that was on her side, and in order for it not only be good but also a blessing.\n",
      "The two princesses were both at war with one another as well…but even so they weren't completely out of their minds about what would happen next after this ordeal ended...and she could see some very strange things coming from them now! Even though Princess Charlotte knew how hard something like an assassination attempt must go against anyone's will or trustworthiness—she didn't\n",
      "<FromString-9361b46c-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>She would have to be a little more careful in her decisions. If she were the only one, it wouldn't matter if they weren\n",
      "\n",
      "The Court of Justice was still waiting for an answer from Princess Charlotte herself and had not even started its work yet! She wasn't going anywhere soon enough!\" \"You are so cute when you're angry at me.\" The voice said softly as Elsa's face turned red with embarrassment...Elsa blinked once again before saying something that made Anna blush harden into tears\n",
      "<FromString-9361b46c-@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@><@>She had a long history of being very stubborn and outspoken. She was the first to admit that she could not stand up for herself, but her voice made it clear why.\"I don't know how I got here,\" said Princess Charlotte with an amused look on his face as he looked at him in surprise.\"\"That's right...Well...\" \"But you didn' mean your sister is going to be dead by now?\" asked Prince Charming looking over from one side only slightly less surprised than usual when they\n"
     ]
    }
   ],
   "source": [
    "# It is also fine to use generate() instead of a pipeline.\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# This also demonstrates multiple return sequences.\n",
    "output = model.generate(input_ids,\n",
    "                        do_sample=True,\n",
    "                        min_length=prompt_len+100,\n",
    "                        max_length=prompt_len+100,\n",
    "                        repetition_penalty=1.7,\n",
    "                        top_p=0.8,\n",
    "                        temperature=0.7,\n",
    "                        use_cache=True,\n",
    "                        return_full_text=True,\n",
    "                        num_return_sequences=3)\n",
    "\n",
    "print(tokenizer.decode(output[0]))\n",
    "print(tokenizer.decode(output[1]))\n",
    "print(tokenizer.decode(output[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you decide to write your own sampler instead of a pipeline,\n",
    "# be aware that the special token logits will be very high.\n",
    "# This is accounted for when using model.generate() or pipelines,\n",
    "# but you will need to exclude special tokens when sampling.\n",
    "\n",
    "output = model(input_ids)"
   ]
  }
 ]
}